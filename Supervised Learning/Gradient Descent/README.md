# Gradient Descent

## Introduction

In mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.

Gradient descent is generally attributed to Cauchy, who first suggested it in 1847. Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.

### - Function requirements
Gradient descent algorithm does not work for all functions. There are two specific requirements. 
A function has to be:
*differentiable
*convex

## File

"Gradient Descent.ipynb" contains the description and code of Gradient Descent algorithm.

## Dataset

Random generated numberical data.

## Reference

Gandhi, R. (2018, May 28). Introduction to machine learning algorithms: Linear regression. Medium. Retrieved May 6, 2022, from https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a 

Wikimedia Foundation. (2022, April 4). Gradient descent. Wikipedia. Retrieved May 6, 2022, from https://en.wikipedia.org/wiki/Gradient_descent 